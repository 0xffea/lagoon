input {
  udp {
    port => 5140
    type => syslog
    queue_size => 5000
    receive_buffer_bytes => 26214400
  }
  lumberjack {
    port => 5044
    ssl_certificate => "certs/lumberjack.cert"
    ssl_key => "certs/lumberjack.key"
    codec => json
  }
}

filter {
  grok {
    match => ["message", "(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"]
  }
  grok {
    match => ["captured_request_headers", "%{URIHOST:request_header_host}\|%{GREEDYDATA:request_header_useragent}"]
  }
  grok {
    match => ["backend_name", "%{NOTSPACE:haproxy_backend}:%{NOTSPACE:openshift_project}:%{NOTSPACE:openshift_route}"]
  }
  grok {
    match => ["server_name", "pod:%{NOTSPACE:openshift_pod}:%{NOTSPACE:openshift_service}:%{NOTSPACE:openshift_pod_ip}:%{NOTSPACE:openshift_pod_port}"]
  }
  if ![openshift_project] {
    mutate {
      add_field => { "openshift_project" => "noproject" }
    }
  }
  # We run automated testing which creates project names in the form of e2e-0000000000-AAAAAAAAAAAAAAAAAAAAAAAAAA
  # we just save them as with the project name 'e2'
  if [openshift_project] =~ /^e2e-\d{10}-\S{26}$/ {
    mutate {
      update => { "openshift_project" => "e2e" }
    }
  }
}

output {
  # stdout { codec => rubydebug }
  elasticsearch {
    user => admin
    password => "${LOGSDB_ADMIN_PASSWORD}"
    hosts => ["${ELASTICSEARCH_URL}"]
    index => "router-logs-%{[openshift_project]}-%{+xxxx.ww}" # index per week
    template => "/usr/share/logstash/templates/router-logs.json"
    template_name => "router-logs"
    template_overwrite => true
  }
}
